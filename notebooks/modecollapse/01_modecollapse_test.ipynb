{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ===============================\n",
    "# MODE COLLAPSE TEST NOTEBOOK\n",
    "# ==============================="
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# âœ… Install Dependencies\n",
    "!pip install -q transformers datasets nltk sentence-transformers tqdm pandas numpy"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "from nltk import ngrams\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Load Dataset (Synthetic Persona-Chat)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"ðŸ“¦ Loading dataset...\")\n",
    "try:\n",
    "    ds = load_dataset(\"google/Synthetic-Persona-Chat\", split=\"train\")\n",
    "    df = pd.DataFrame(ds)\n",
    "except Exception as e:\n",
    "    print(\"âš ï¸ Couldn't load from hub:\", e)\n",
    "    print(\"Trying manual CSV load instead...\")\n",
    "    df = pd.read_csv(\"hf://datasets/google/Synthetic-Persona-Chat/data/Synthetic-Persona-Chat_train.csv\")\n",
    "\n",
    "print(\"\\nâœ… Dataset loaded successfully!\")\n",
    "print(\"Available columns:\", df.columns.tolist())\n",
    "print(df.head(2))\n",
    "\n",
    "# Automatically detect the most likely dialogue column\n",
    "dialogue_col = None\n",
    "for c in df.columns:\n",
    "    if any(k in c.lower() for k in [\"dialog\", \"conversation\", \"prompt\", \"text\", \"utterance\"]):\n",
    "        dialogue_col = c\n",
    "        break\n",
    "\n",
    "if not dialogue_col:\n",
    "    raise ValueError(\"âŒ No suitable dialogue column found in dataset. Check column names above.\")\n",
    "\n",
    "# Use a subset for testing\n",
    "prompts = df[dialogue_col].astype(str).head(100).tolist()\n",
    "print(f\"\\nUsing column '{dialogue_col}' with {len(prompts)} prompts.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Load Models"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\nðŸ§  Loading text-generation models...\")\n",
    "model_names = {\n",
    "    \"GPT-2\": \"gpt2\",\n",
    "    \"DistilGPT-2\": \"distilgpt2\"\n",
    "}\n",
    "models = {}\n",
    "\n",
    "for name, model_id in model_names.items():\n",
    "    try:\n",
    "        print(f\"â†’ Loading {name}...\")\n",
    "        models[name] = pipeline(\"text-generation\", model=model_id, device_map=\"auto\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Failed to load {name}: {e}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Generate Outputs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_responses(model, prompts, max_new_tokens=40):\n",
    "    responses = []\n",
    "    for p in tqdm(prompts, desc=\"ðŸ” Generating\", leave=False):\n",
    "        try:\n",
    "            out = model(p, max_new_tokens=max_new_tokens, do_sample=True, temperature=0.8)\n",
    "            responses.append(out[0][\"generated_text\"])\n",
    "        except Exception as e:\n",
    "            responses.append(\"\")\n",
    "    return responses\n",
    "\n",
    "responses_dict = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nðŸ—£ Generating responses with {name}...\")\n",
    "    responses_dict[name] = generate_responses(model, prompts)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Diversity Metrics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def distinct_n(texts, n):\n",
    "    all_ngrams = [ng for t in texts for ng in ngrams(t.split(), n)]\n",
    "    if not all_ngrams:\n",
    "        return 0.0\n",
    "    return len(set(all_ngrams)) / len(all_ngrams)\n",
    "\n",
    "def self_bleu(texts, sample_size=30):\n",
    "    sample_texts = texts[:sample_size]\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    scores = []\n",
    "    for i in range(len(sample_texts)):\n",
    "        refs = [t.split() for j, t in enumerate(sample_texts) if j != i]\n",
    "        if len(refs) == 0: continue\n",
    "        score = sentence_bleu(refs, sample_texts[i].split(), smoothing_function=smoothie)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores) if scores else 0.0\n",
    "\n",
    "print(\"\\nðŸ“Š Calculating diversity metrics...\")\n",
    "\n",
    "metrics = {}\n",
    "for name, responses in responses_dict.items():\n",
    "    d1 = distinct_n(responses, 1)\n",
    "    d2 = distinct_n(responses, 2)\n",
    "    sb = self_bleu(responses)\n",
    "    metrics[name] = {\"Distinct-1\": d1, \"Distinct-2\": d2, \"Self-BLEU\": sb}\n",
    "    print(f\"\\n{name} Metrics:\")\n",
    "    print(f\"Distinct-1: {d1:.4f}, Distinct-2: {d2:.4f}, Self-BLEU: {sb:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Embedding Variance (Semantic Diversity)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\nðŸ§© Computing embedding variance (semantic diversity)...\")\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def embedding_variance(texts):\n",
    "    embeddings = embedder.encode(texts, show_progress_bar=False)\n",
    "    return np.mean(np.var(embeddings, axis=0))\n",
    "\n",
    "for name, responses in responses_dict.items():\n",
    "    var = embedding_variance(responses)\n",
    "    metrics[name][\"EmbeddingVariance\"] = var\n",
    "    print(f\"{name} Embedding Variance: {var:.6f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Results & Mode Collapse Analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\n==================== MODE COLLAPSE ANALYSIS ====================\")\n",
    "print(\"Indicators of mode collapse:\")\n",
    "print(\"- High Self-BLEU â†’ Less diversity\")\n",
    "print(\"- Low Distinct-n â†’ Repetitive wording\")\n",
    "print(\"- Low Embedding Variance â†’ Semantic similarity\\n\")\n",
    "\n",
    "for name, vals in metrics.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    for k, v in vals.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "# Simple thresholds for interpretation\n",
    "def detect_collapse(m):\n",
    "    return m[\"Self-BLEU\"] > 0.8 or m[\"Distinct-1\"] < 0.05 or m[\"EmbeddingVariance\"] < 0.005\n",
    "\n",
    "print(\"\\n==================== CONCLUSION ====================\")\n",
    "collapse_flags = {name: detect_collapse(vals) for name, vals in metrics.items()}\n",
    "for name, flag in collapse_flags.items():\n",
    "    if flag:\n",
    "        print(f\"âš ï¸ {name}: Possible mode collapse detected.\")\n",
    "    else:\n",
    "        print(f\"âœ… {name}: No strong signs of mode collapse.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"\\nðŸ“„ Summary Table:\")\n",
    "summary = pd.DataFrame(metrics).T\n",
    "summary"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

