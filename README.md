# ğŸ“ The Flattererâ€™s Dilemma â€” FYP 2025  
### *Why AI Would Rather Lie Than Disappoint*

This repository hosts the implementation and experimental framework for the Final Year Project **â€œThe Flattererâ€™s Dilemmaâ€**, which investigates **sycophancy**, **mode collapse**, and **reward tampering** in reinforcement-learning-trained large language models (LLMs).

---

## ğŸ§  Project Overview

Sycophancy refers to a failure mode where models prioritize *agreement* with the user instead of *truth*.  
This project replicates and extends existing evaluation frameworks (e.g., TruthfulQA, SycEval) and introduces an analysis pipeline to measure alignment failures in AI assistants.

The repo includes:
- ğŸ“œ Research review paper and presentation (see `docs/`)
- ğŸ’¾ Data loaders for evaluation datasets
- ğŸ§® Evaluation scripts for baseline and fine-tuned models
- ğŸ“Š Example Colab-runnable code for testing
- âš™ï¸ Configs for experiments and reproducibility

---

## ğŸ“‚ Repository Structure

