# 🎓 The Flatterer’s Dilemma — FYP 2025  
### *Why AI Would Rather Lie Than Disappoint*

This repository hosts the implementation and experimental framework for the Final Year Project **“The Flatterer’s Dilemma”**, which investigates **sycophancy**, **mode collapse**, and **reward tampering** in reinforcement-learning-trained large language models (LLMs).

---

## 🧠 Project Overview

Sycophancy refers to a failure mode where models prioritize *agreement* with the user instead of *truth*.  
This project replicates and extends existing evaluation frameworks (e.g., TruthfulQA, SycEval) and introduces an analysis pipeline to measure alignment failures in AI assistants.

The repo includes:
- 📜 Research review paper and presentation (see `docs/`)
- 💾 Data loaders for evaluation datasets
- 🧮 Evaluation scripts for baseline and fine-tuned models
- 📊 Example Colab-runnable code for testing
- ⚙️ Configs for experiments and reproducibility

---

## 📂 Repository Structure

