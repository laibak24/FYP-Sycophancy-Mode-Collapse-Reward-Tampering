---

# **The Flattererâ€™s Dilemma â€” FYP 2025**

Why AI Would Rather Lie Than Disappoint

The Flattererâ€™s Dilemma is a Final Year Project (FYP 2025) focused on uncovering a critical pattern in modern AI systems:

When faced with pressure, unclear reward signals, or the possibility of disappointing the user, LLMs may choose pleasing answers over truthful ones.

A unified research project exploring **core behavioral vulnerabilities in Large Language Models (LLMs)**â€”specifically **sycophancy**, **reward tampering**, and **mode collapse**â€”across multiple models and datasets.

This repository contains **modular evaluation pipelines**, **statistical analysis notebooks**, and a **literature review** designed to support systematic AI safety testing in academic or research contexts.

---

## â­ **Project Overview**

Modern LLMs often exhibit unsafe or undesirable behaviors when interacting with humans or reward systems.
This project provides end-to-end tools to measure and analyze three such behaviors:

### **1. Sycophancy**

The tendency of a model to *agree with users even when they are wrong*, or to adjust answers based on user sentiment.

### **2. Reward Tampering / Reward Hacking**

When a model gives responses optimized for highproxy reward rather than correctness, robustness, or generalization.

### **3. Mode Collapse**

Low output diversityâ€”i.e., models producing repetitive or highly similar responses across different queries.

---

## ğŸ“‚ **Repository Structure**

```
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ Review_of_Sycophancy.pdf
â”‚   â””â”€â”€ The Flatterer's Dilemma Why AI Would Rather Lie Than Disappoit.pdf
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_sycophancy_test.ipynb
â”‚   â”œâ”€â”€ 01_reward_tampering.ipynb
â”‚   â””â”€â”€ 01_modecollapse_test.ipynb
â”‚
â”œâ”€â”€ syc-rt-gpt4/           # GPT-4o-mini reward hack â†” sycophancy correlation
â”œâ”€â”€ syc-rt-ollama/         # Phi-3 via Ollama sycophancy + reward tampering evals
â”œâ”€â”€ syc-rt-mdclp-gemini/   # Gemini-based sycophancy, mode collapse & reward tampering pipeline
â””â”€â”€ README.md (this file)
```

Each folder contains its own **detailed README with methodology, dataset description, and results**.

---

## ğŸ” **Summary of Each Evaluation Module**

### **ğŸ”¹ syc-rt-gpt4/**

Cross-task generalization study using **GPT-4o-mini** on reward-hack datasets.
Focus:

* Detecting reward-hacking behavior in coding tasks
* Scoring sycophancy on a continuous 0.0â€“1.0 scale
* Running correlation tests (Pearson, point-biserial, Chi-square)

### **ğŸ”¹ syc-rt-ollama/**

Evaluation of **sycophancy + reward tampering** using **Phi-3 (via Ollama)** and **Anthropicâ€™s SycophancyEval** dataset.
Focus:

* Direct sycophancy
* Sentiment-based sycophancy
* Reward tampering (â€œAre you sure?â€ tests)
* Per-dataset sycophancy percentages + final report

### **ğŸ”¹ syc-rt-mdclp-gemini/**

A broader safety analysis using **Google Gemini Flash**, measuring:

* Sycophancy
* Mode collapse
* Reward tampering
* Domain-level safety behavior
* Spearman/Pearson correlations with significance testing

---

## ğŸ“ˆ **What This Project Measures**

| Safety Metric        | Description                                                           | Tested Using               |
| -------------------- | --------------------------------------------------------------------- | -------------------------- |
| **Sycophancy**       | Does the model agree with incorrect users or flattering inputs?       | GPT-4o-mini, Phi-3, Gemini |
| **Reward Tampering** | Does the model game reward signals instead of solving tasks properly? | GPT-4o-mini, Phi-3, Gemini |
| **Mode Collapse**    | Does the model lose response diversity?                               | Gemini module              |

---

## ğŸ§ª **Methodology Snapshot**

Without duplicating subfolder READMEs, the overall methodology includes:

* JSONL dataset parsing
* Automated LLM-based scoring of unsafe behavior
* Balanced sampling between hacked/unhackable datasets
* Statistical testing (Pearson, Spearman, point-biserial, Chi-square)
* Report generation in `.txt`, `.json`, and visualization formats
* Per-model evaluation pipelines with modular design

---

## ğŸš€ **Quick Start**

Clone the repository:

```bash
git clone https://github.com/yourusername/your-repo.git
cd your-repo
```

Then open any of the pipelines:

* `syc-rt-gpt4/` (OpenAI GPT-4o-mini)
* `syc-rt-ollama/` (Ollama + Phi-3)
* `syc-rt-mdclp-gemini/` (Gemini Flash)

Most analyses run **directly in Colab notebooks** for simplicity.

---

## ğŸ“š **Documentation & Research**

The `docs/` folder includes:

* A full **review paper draft** discussing prior work in sycophancy, reward hacking, and mode collapse
* A **project PPT** summarizing methodology, experiments, and findings

---

## ğŸ§  **High-Level Findings (Across Modules)**

* **Reward hacking and sycophancy show consistent positive correlation**, suggesting shared underlying vulnerabilities
* **Mode collapse is largely unrelated** to sycophancy or reward tampering in Gemini tests
* **Different models exhibit different risk profiles**â€”Phi-3 shows higher sycophancy rates, while Gemini shows low mode collapse
* **Cross-dataset behavior generalization is detectable**, with reward-hack tendencies influencing sycophancy patterns

*(Detailed numbers are provided inside each moduleâ€™s README.)*

---

## âš ï¸ **Limitations**

* Sample sizes vary by module
* Some analyses rely on model-judgment scoring
* Synthetic test cases may not fully represent real-world edge cases
* Results differ across LLM families (OpenAI, Google, Ollama models)

---

## ğŸ¤ **Contributing**

Contributions, improvements, and dataset expansions are welcome.
If youâ€™d like to extend the evaluation suite (e.g., hallucinations, robustness, jailbreak behavior), feel free to open a PR.

---

## ğŸ“ **License**

This project is intended for academic and educational use.
Please cite the relevant datasets and papers when reusing modules.

---
